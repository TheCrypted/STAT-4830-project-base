{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1e25156-ab19-437e-be80-a26a47a7b012",
   "metadata": {},
   "source": [
    "## Graph Neural Network Model for Route Optimization\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b647fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch\n",
    "!pip install osmnx\n",
    "!pip install pandas numpy osmnx geopandas shapely matplotlib scikit-learn pathlib gymnasium torch torch_geometric torch_sparse torch_scatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e31ac12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import osmnx as ox\n",
    "import networkx as nx\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GINEConv\n",
    "from torch.nn import Linear, ReLU, Dropout, LayerNorm, GRU\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5101840b",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\", message=\"invalid value encountered in cast\", category=RuntimeWarning)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "temp_cols = ['VehId','Trip','Timestamp(ms)','Latitude[deg]','Longitude[deg]','Vehicle Speed[km/h]','MAF[g/sec]','Engine RPM[RPM]','Absolute Load[%]','Speed Limit[km/h]']\n",
    "chunksize = 200_000\n",
    "cleaned = []\n",
    "lat_mins, lat_maxs, lon_mins, lon_maxs = [], [], [], []\n",
    "dtypes = {'VehId':'int32','Trip':'int32'}\n",
    "\n",
    "def load_ev_data(path):\n",
    "    for chunk in pd.read_csv(path, usecols=temp_cols, dtype=dtypes, chunksize=chunksize,\n",
    "                              low_memory=False, on_bad_lines='warn'):\n",
    "        # numeric conversion\n",
    "        for col in temp_cols[2:]:\n",
    "            if col in chunk.columns:\n",
    "                num = pd.to_numeric(chunk[col], errors='coerce').fillna(0)\n",
    "                chunk[col] = num.astype('float32') if col!='Timestamp(ms)' else num.astype('int64')\n",
    "        # filter out zeros\n",
    "        chunk = chunk[(chunk['Latitude[deg]']!=0)&(chunk['Longitude[deg]']!=0)]\n",
    "        if chunk.empty: continue\n",
    "        lat_mins.append(chunk['Latitude[deg]'].min()); lat_maxs.append(chunk['Latitude[deg]'].max())\n",
    "        lon_mins.append(chunk['Longitude[deg]'].min()); lon_maxs.append(chunk['Longitude[deg]'].max())\n",
    "        cleaned.append(chunk.sort_values(['VehId','Trip','Timestamp(ms)']))\n",
    "\n",
    "file_paths = glob.glob('./eVED/*.csv')\n",
    "if not file_paths:\n",
    "    raise FileNotFoundError(\"No CSV files found in ./eVED/\")\n",
    "for p in file_paths:\n",
    "    load_ev_data(p)\n",
    "ev_df = pd.concat(cleaned, ignore_index=True)\n",
    "print(f\"Loaded EV data: {ev_df.shape[0]} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bc1560",
   "metadata": {},
   "outputs": [],
   "source": [
    "LAT_MIN, LAT_MAX = min(lat_mins), max(lat_maxs)\n",
    "LNG_MIN, LNG_MAX = min(lon_mins), max(lon_maxs)\n",
    "def normalize_coords(lat, lon):\n",
    "    nl = (lat - LAT_MIN)/(LAT_MAX-LAT_MIN) if LAT_MAX!=LAT_MIN else 0\n",
    "    ml = (lon - LNG_MIN)/(LNG_MAX-LNG_MIN) if LNG_MAX!=LNG_MIN else 0\n",
    "    return np.clip(nl,0,1), np.clip(ml,0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5178f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "center_lat, center_lon = (LAT_MIN+LAT_MAX)/2, (LNG_MIN+LNG_MAX)/2\n",
    "print(\"Downloading OSMnx graph...\")\n",
    "G_nx = ox.graph_from_point((center_lat, center_lon), dist=30000, network_type='drive')\n",
    "node_id_map = {nid:i for i,nid in enumerate(G_nx.nodes())}\n",
    "# node features\n",
    "node_feats = []\n",
    "for nid,data in G_nx.nodes(data=True):\n",
    "    nl,ml = normalize_coords(data['y'], data['x']); node_feats.append([nl, ml])\n",
    "x = torch.tensor(node_feats, dtype=torch.float)\n",
    "# edge features\n",
    "e_u,e_v,edge_feats = [],[],[]\n",
    "lengths, speeds = [], []\n",
    "for u,v,data in G_nx.edges(data=True):\n",
    "    lengths.append(data.get('length',0.0))\n",
    "    ms = data.get('maxspeed',0)\n",
    "    if isinstance(ms,list): ms = ms[0]\n",
    "    try: speeds.append(float(str(ms).split()[0]))\n",
    "    except: speeds.append(0.0)\n",
    "max_len = max(lengths) or 1.0; MAX_SP=130.0\n",
    "for i,(u,v) in enumerate(G_nx.edges(data=False)):\n",
    "    ui,vi = node_id_map[u], node_id_map[v]\n",
    "    e_u.append(ui); e_v.append(vi)\n",
    "    nl = lengths[i]/max_len; ns = speeds[i]/MAX_SP\n",
    "    edge_feats.append([nl, np.clip(ns,0,1)])\n",
    "edge_index = torch.tensor([e_u,e_v], dtype=torch.long)\n",
    "edge_attr = torch.tensor(edge_feats, dtype=torch.float)\n",
    "graph = Data(x=x, edge_index=edge_index, edge_attr=edge_attr)\n",
    "print(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab02263",
   "metadata": {},
   "outputs": [],
   "source": [
    "ev_df['node_osm'] = ox.nearest_nodes(G_nx, X=ev_df['Longitude[deg]'], Y=ev_df['Latitude[deg]'])\n",
    "ev_df['node_idx'] = ev_df['node_osm'].map(node_id_map)\n",
    "ev_df.dropna(subset=['node_idx'], inplace=True)\n",
    "ev_df['node_idx'] = ev_df['node_idx'].astype(int)\n",
    "ev_df.sort_values(['VehId','Trip','Timestamp(ms)'], inplace=True)\n",
    "ev_df['duration_s'] = ev_df.groupby(['VehId','Trip'])['Timestamp(ms)'].diff().fillna(0)/1000\n",
    "ev_df['step_energy'] = ev_df['MAF[g/sec]'] * ev_df['duration_s']\n",
    "trips=[]\n",
    "for _,g in ev_df.groupby(['VehId','Trip']):\n",
    "    if len(g)<2: continue\n",
    "    s = int(g.iloc[0].node_idx); d = int(g.iloc[-1].node_idx)\n",
    "    e_sum = g.step_energy.sum()\n",
    "    if s!=d and e_sum>0: trips.append({'source':s,'destination':d,'true_energy':e_sum})\n",
    "processed_trips_df = pd.DataFrame(trips)\n",
    "print(f\"Processed {len(processed_trips_df)} trips\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852a5a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "LEARNING_RATE = 5e-4\n",
    "BATCH_SIZE = 16  # Match your DataLoader's batch size\n",
    "NUM_EPOCHS = 30\n",
    "VALIDATION_SPLIT = 0.15\n",
    "MODEL_SAVE_PATH = './best_path_energy_predictor.pth'\n",
    "\n",
    "class PathTripDataset(Dataset):\n",
    "    def __init__(self, trip_df, graph, G_nx, idx2osm):\n",
    "        self.trips = trip_df.reset_index(drop=True)\n",
    "        self.graph = graph\n",
    "        self.G_nx = G_nx\n",
    "        self.idx2osm = idx2osm\n",
    "        # Cache: (osmid_src, osmid_dst) -> torch.LongTensor of edge indices\n",
    "        self._path_cache = {}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.trips)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        r = self.trips.iloc[i]\n",
    "        s, d = int(r.source), int(r.destination)\n",
    "        os_s, os_d = self.idx2osm[s], self.idx2osm[d]\n",
    "        key = (os_s, os_d)\n",
    "\n",
    "        # 1) lookup in cache\n",
    "        if key in self._path_cache:\n",
    "            edge_idxs = self._path_cache[key]\n",
    "        else:\n",
    "            # 2) compute shortest path nodes\n",
    "            try:\n",
    "                path_nodes = nx.shortest_path(self.G_nx, os_s, os_d, weight='length')\n",
    "            except (nx.NetworkXNoPath, nx.NodeNotFound):\n",
    "                path_nodes = []\n",
    "\n",
    "            # 3) map node pairs to edge_index positions\n",
    "            idxs = []\n",
    "            for u_osm, v_osm in zip(path_nodes[:-1], path_nodes[1:]):\n",
    "                for j, (u, v) in enumerate(zip(self.graph.edge_index[0], self.graph.edge_index[1])):\n",
    "                    if self.idx2osm[u] == u_osm and self.idx2osm[v] == v_osm:\n",
    "                        idxs.append(j)\n",
    "                        break\n",
    "\n",
    "            edge_idxs = torch.tensor(idxs, dtype=torch.long)\n",
    "            # 4) store in cache\n",
    "            self._path_cache[key] = edge_idxs\n",
    "\n",
    "        # 5) log-scale energy\n",
    "        energy_log = np.log1p(r.true_energy)\n",
    "        # Note: Removed the device placement here to avoid conflicts with training loop\n",
    "        return edge_idxs, torch.tensor(energy_log, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46d3abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNPathEnergyPredictor(nn.Module):\n",
    "    def __init__(self, node_dim, edge_dim, hidden_dim=128, rnn_hidden=128, num_layers=4, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.node_emb = Linear(node_dim, hidden_dim)\n",
    "        self.edge_emb = Linear(edge_dim, hidden_dim)\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.norms = nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            emlp = nn.Sequential(Linear(hidden_dim, hidden_dim), ReLU(), Linear(hidden_dim, hidden_dim))\n",
    "            self.convs.append(GINEConv(emlp, edge_dim=hidden_dim))\n",
    "            self.norms.append(LayerNorm(hidden_dim))\n",
    "        self.rnn = GRU(input_size=3*hidden_dim, hidden_size=rnn_hidden, batch_first=True)\n",
    "        self.decoder = nn.Sequential(Linear(rnn_hidden, hidden_dim), ReLU(), Dropout(dropout), Linear(hidden_dim,1))\n",
    "    def forward(self, graph, paths):\n",
    "        x = self.node_emb(graph.x)\n",
    "        ea = self.edge_emb(graph.edge_attr)\n",
    "        for conv,nrm in zip(self.convs,self.norms):\n",
    "            xr = x\n",
    "            x = conv(x, graph.edge_index, ea)\n",
    "            x = nrm(x); x = F.relu(x)\n",
    "            x = x + xr\n",
    "        emb = x\n",
    "        seqs=[]\n",
    "        for eidx in paths:\n",
    "            edges = graph.edge_index[:, eidx]\n",
    "            u,v = edges[0], edges[1]\n",
    "            s_emb = emb[u]; d_emb = emb[v]; ef = ea[eidx]\n",
    "            seqs.append(torch.cat([s_emb, ef, d_emb], dim=1))\n",
    "        packed = pack_padded_sequence(pad_sequence(seqs, batch_first=True), [len(s) for s in seqs], batch_first=True, enforce_sorted=False)\n",
    "        _, h = self.rnn(packed)\n",
    "        preds = self.decoder(h.squeeze(0)).squeeze(1)\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584b939f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Splitting trip data into training and validation sets...\")\n",
    "train_trips_df, val_trips_df = train_test_split(\n",
    "    processed_trips_df,\n",
    "    test_size=VALIDATION_SPLIT,\n",
    "    random_state=42\n",
    ")\n",
    "print(f\"Training trips: {len(train_trips_df)}, Validation trips: {len(val_trips_df)}\")\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "idx2osm = {v: k for k, v in node_id_map.items()}\n",
    "train_dataset = PathTripDataset(train_trips_df, graph, G_nx, idx2osm)\n",
    "val_dataset = PathTripDataset(val_trips_df, graph, G_nx, idx2osm)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE*2, shuffle=False)\n",
    "\n",
    "# Initialize model, optimizer, and loss function\n",
    "model = GNNPathEnergyPredictor(graph.x.shape[1], graph.edge_attr.shape[1]).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# Prepare graph data (move to device once)\n",
    "print(f\"Moving graph data to device {DEVICE}...\")\n",
    "try:\n",
    "    prepared_graph = graph.to(DEVICE)\n",
    "    print(\"Graph data successfully moved.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error moving graph data to device {DEVICE}: {e}\")\n",
    "    print(\"Exiting.\")\n",
    "    exit()\n",
    "\n",
    "# Training tracking variables\n",
    "best_val_loss = float('inf')\n",
    "history = {'train_loss': [], 'val_loss': []}\n",
    "\n",
    "# Training loop\n",
    "print(f\"\\n--- Starting Training for {NUM_EPOCHS} Epochs ---\")\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # --- Training Phase ---\n",
    "    model.train()\n",
    "    total_train_loss = 0.0\n",
    "    train_batches = 0\n",
    "    train_progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} [Train]\", leave=False)\n",
    "    \n",
    "    for edge_idxs, energy in train_progress_bar:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Move batch data to device\n",
    "        edge_idxs = edge_idxs.to(DEVICE)\n",
    "        energy = energy.to(DEVICE)\n",
    "        \n",
    "        # Forward pass\n",
    "        predictions = model(prepared_graph, edge_idxs)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = loss_fn(predictions, energy)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_train_loss += loss.item()\n",
    "        train_batches += 1\n",
    "        train_progress_bar.set_postfix({'loss': f\"{loss.item():.4f}\"})\n",
    "    \n",
    "    avg_train_loss = total_train_loss / train_batches if train_batches > 0 else 0.0\n",
    "    history['train_loss'].append(avg_train_loss)\n",
    "    \n",
    "    # --- Validation Phase ---\n",
    "    model.eval()\n",
    "    total_val_loss = 0.0\n",
    "    val_batches = 0\n",
    "    val_progress_bar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} [Val]\", leave=False)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for edge_idxs, energy in val_progress_bar:\n",
    "            # Move batch data to device\n",
    "            edge_idxs = edge_idxs.to(DEVICE)\n",
    "            energy = energy.to(DEVICE)\n",
    "            \n",
    "            # Forward pass\n",
    "            predictions = model(prepared_graph, edge_idxs)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = loss_fn(predictions, energy)\n",
    "            total_val_loss += loss.item()\n",
    "            val_batches += 1\n",
    "            val_progress_bar.set_postfix({'loss': f\"{loss.item():.4f}\"})\n",
    "    \n",
    "    avg_val_loss = total_val_loss / val_batches if val_batches > 0 else 0.0\n",
    "    history['val_loss'].append(avg_val_loss)\n",
    "    end_time = time.time()\n",
    "    epoch_duration = end_time - start_time\n",
    "    \n",
    "    # --- Epoch Summary and Model Saving ---\n",
    "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS} | Duration: {epoch_duration:.2f}s\")\n",
    "    print(f\"  Train Loss: {avg_train_loss:.6f}\")\n",
    "    print(f\"  Val Loss:   {avg_val_loss:.6f}\")\n",
    "    \n",
    "    # Save the model if validation loss improved\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        # Save the model state dictionary\n",
    "        torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
    "        print(f\"  Validation loss improved. Saved model to {MODEL_SAVE_PATH}\")\n",
    "    else:\n",
    "        print(\"  Validation loss did not improve.\")\n",
    "\n",
    "print(\"\\n--- Training Complete ---\")\n",
    "print(f\"Best Validation Loss: {best_val_loss:.6f}\")\n",
    "print(f\"Best model saved to: {MODEL_SAVE_PATH}\")\n",
    "\n",
    "# Optional: Load best model for inference\n",
    "best_model = GNNPathEnergyPredictor(graph.x.shape[1], graph.edge_attr.shape[1]).to(DEVICE)\n",
    "best_model.load_state_dict(torch.load(MODEL_SAVE_PATH))\n",
    "print(\"Loaded best model for inference.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
